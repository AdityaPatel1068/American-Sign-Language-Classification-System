{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 0 classes.\n",
      "Found 0 images belonging to 0 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ap2935\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import string\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define constants\n",
    "input_shape = (256, 256, 1)  # Image dimensions and grayscale\n",
    "original_dataset_dir = r\"C:\\Users\\ap2935\\Desktop\\asl_dataset\\PRO\"  \n",
    "split_dataset_dir = r\"C:\\Users\\ap2935\\Desktop\\asl_dataset\"  \n",
    "train_dir = os.path.join(split_dataset_dir, \"train\")\n",
    "validation_dir = os.path.join(split_dataset_dir, \"validation\")\n",
    "test_dir = os.path.join(split_dataset_dir, \"test\")\n",
    "\n",
    "# Function to split the dataset\n",
    "def split_dataset(original_dir, split_dir, train_ratio=0.7, validation_ratio=0.2, test_ratio=0.1):\n",
    "    if not os.path.exists(split_dir):\n",
    "        os.makedirs(split_dir)\n",
    "    \n",
    "    for subset in [\"train\", \"validation\", \"test\"]:\n",
    "        os.makedirs(os.path.join(split_dir, subset), exist_ok=True)\n",
    "    \n",
    "    for class_name in os.listdir(original_dir):\n",
    "        class_dir = os.path.join(original_dir, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "\n",
    "        files = os.listdir(class_dir)\n",
    "        random.shuffle(files)\n",
    "        total_files = len(files)\n",
    "\n",
    "        # Split indices\n",
    "        train_end = int(total_files * train_ratio)\n",
    "        validation_end = train_end + int(total_files * validation_ratio)\n",
    "\n",
    "        # Create splits\n",
    "        train_files = files[:train_end]\n",
    "        validation_files = files[train_end:validation_end]\n",
    "        test_files = files[validation_end:]\n",
    "\n",
    "        # Copy files to respective directories\n",
    "        for file in train_files:\n",
    "            os.makedirs(os.path.join(split_dir, \"train\", class_name), exist_ok=True)\n",
    "            shutil.copy(os.path.join(class_dir, file), os.path.join(split_dir, \"train\", class_name, file))\n",
    "        for file in validation_files:\n",
    "            os.makedirs(os.path.join(split_dir, \"validation\", class_name), exist_ok=True)\n",
    "            shutil.copy(os.path.join(class_dir, file), os.path.join(split_dir, \"validation\", class_name, file))\n",
    "        for file in test_files:\n",
    "            os.makedirs(os.path.join(split_dir, \"test\", class_name), exist_ok=True)\n",
    "            shutil.copy(os.path.join(class_dir, file), os.path.join(split_dir, \"test\", class_name, file))\n",
    "\n",
    "# Split the dataset\n",
    "split_dataset(original_dataset_dir, split_dataset_dir)\n",
    "\n",
    "# Function to apply data augmentation\n",
    "def apply_data_augmentation(train_dir):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    return train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(input_shape[0], input_shape[1]),\n",
    "        batch_size=32,\n",
    "        color_mode='grayscale',\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "\n",
    "# Validation generator\n",
    "def create_validation_generator(validation_dir):\n",
    "    validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    return validation_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(input_shape[0], input_shape[1]),\n",
    "        batch_size=32,\n",
    "        color_mode='grayscale',\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "\n",
    "# Test generator\n",
    "def create_test_generator(test_dir):\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    return test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(input_shape[0], input_shape[1]),\n",
    "        batch_size=32,\n",
    "        color_mode='grayscale',\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "\n",
    "# Build a more complex CNN model\n",
    "def build_cnn_model(num_classes):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Block 1\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.25))  # Regularization\n",
    "\n",
    "    # Block 2\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "\n",
    "    # Block 3\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    # Global Average Pooling\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "    # Fully Connected Layer\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function to compile the model\n",
    "def compile_model(model):\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, train_generator, validation_generator, epochs=10):\n",
    "    model.fit(train_generator, validation_data=validation_generator, epochs=epochs)\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, test_generator):\n",
    "    return model.evaluate(test_generator)\n",
    "\n",
    "# Apply data augmentation\n",
    "train_generator = apply_data_augmentation(train_dir)\n",
    "validation_generator = create_validation_generator(validation_dir)\n",
    "test_generator = create_test_generator(test_dir)\n",
    "\n",
    "# Build the CNN model\n",
    "cnn_model = build_cnn_model(num_classes=len(train_generator.class_indices))\n",
    "\n",
    "# Compile the model\n",
    "compile_model(cnn_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ap2935\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must provide at least one structure",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_model(cnn_model, train_generator, validation_generator, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(cnn_model, test_generator)\n",
      "Cell \u001b[1;32mIn[14], line 147\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_generator, validation_generator, epochs)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(model, train_generator, validation_generator, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m--> 147\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(train_generator, validation_data\u001b[38;5;241m=\u001b[39mvalidation_generator, epochs\u001b[38;5;241m=\u001b[39mepochs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\tree\\optree_impl.py:76\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structures)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`func` must be callable. Received: func=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m structures:\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust provide at least one structure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m     78\u001b[0m     assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: Must provide at least one structure"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(cnn_model, train_generator, validation_generator, epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = evaluate_model(cnn_model, test_generator)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "evaluation_result = evaluate_model(cnn_model, test_generator)\n",
    "print(\"Evaluation Result:\", evaluation_result)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = cnn_model.predict(test_generator)\n",
    "\n",
    "# Get the class labels\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Loop through each prediction and print the class label with its probability\n",
    "for i, prediction in enumerate(predictions):\n",
    "    predicted_class_index = tf.argmax(prediction).numpy()\n",
    "    predicted_class_label = class_labels[predicted_class_index]\n",
    "    predicted_probability = prediction[predicted_class_index]\n",
    "\n",
    "    print(f\"Sample {i + 1}: Predicted Class - {predicted_class_label}, Probability - {predicted_probability:.4f}\")\n",
    "\n",
    "# Save the model to a specified path\n",
    "save_path = '/Users/aditya/Documents/GitHub/ASL/MODEL17' \n",
    "cnn_model.save(save_path)\n",
    "print(f\"Model saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import rembg\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import Image as IPImage, display\n",
    "\n",
    "# Function to capture an image from the default camera, remove the background, and preprocess it\n",
    "def capture_and_preprocess():\n",
    "    # Open the default camera (camera index 0)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Allow the camera to adjust (you may need to adjust the delay based on your camera)\n",
    "    cv2.waitKey(1000)\n",
    "\n",
    "    # Read a frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Release the camera capture\n",
    "    cap.release()\n",
    "\n",
    "    if ret:\n",
    "        # Convert the OpenCV frame to a PIL Image\n",
    "        pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # Use rembg to remove the background\n",
    "        with rembg.remove(pil_image) as result:\n",
    "            # Convert the result to an OpenCV format\n",
    "            sign = cv2.cvtColor(np.array(result), cv2.COLOR_RGBA2BGRA)\n",
    "\n",
    "        # Convert the image to grayscale\n",
    "        grayscale_image = cv2.cvtColor(sign, cv2.COLOR_BGRA2GRAY)\n",
    "\n",
    "        # Resize the image to the target size (256x256)\n",
    "        target_size = (256, 256)\n",
    "        resized_image = cv2.resize(grayscale_image, target_size)\n",
    "\n",
    "        # Remove random noise (salt and pepper noise)\n",
    "        noisy_image = Image.fromarray(resized_image)\n",
    "        noisy_image = noisy_image.point(lambda p: p + random.choice([-50, 0, 50]) if random.random() < 0.05 else p)\n",
    "\n",
    "        # Convert the noisy image back to NumPy array\n",
    "        noisy_image_np = np.array(noisy_image)\n",
    "\n",
    "        return frame, noisy_image_np\n",
    "    else:\n",
    "        print(\"Failed to capture an image.\")\n",
    "        return None\n",
    "\n",
    "from gtts import gTTS\n",
    "from IPython.display import Audio\n",
    "from io import BytesIO\n",
    "\n",
    "def text_to_speech(text):\n",
    "    # Create a gTTS object with the desired text\n",
    "    tts = gTTS(text=text, lang='en')\n",
    "\n",
    "    # Save the generated speech to a BytesIO object\n",
    "    audio_stream = BytesIO()\n",
    "    tts.write_to_fp(audio_stream)\n",
    "    audio_stream.seek(0)\n",
    "\n",
    "    # Display the generated audio\n",
    "    return Audio(data=audio_stream.read(), autoplay=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated and will be removed in a future release\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import rembg\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Function to capture an image from the default camera, remove the background, and preprocess it\n",
    "def capture_and_preprocess():\n",
    "    # Open the default camera (camera index 0)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Allow the camera to adjust (you may need to adjust the delay based on your camera)\n",
    "    cv2.waitKey(1000)\n",
    "\n",
    "    # Read a frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Release the camera capture\n",
    "    cap.release()\n",
    "\n",
    "    if ret:\n",
    "        # Convert the OpenCV frame to a PIL Image\n",
    "        pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # Use rembg to remove the background\n",
    "        result = rembg.remove(pil_image)\n",
    "        sign = cv2.cvtColor(np.array(result), cv2.COLOR_RGBA2GRAY)\n",
    "\n",
    "        # Resize the image to the target size (256x256)\n",
    "        target_size = (256, 256)\n",
    "        resized_image = cv2.resize(sign, target_size)\n",
    "\n",
    "        # Denoise the image (optional)\n",
    "        denoised_image = cv2.fastNlMeansDenoising(resized_image, None, 30, 7, 21)\n",
    "\n",
    "        return frame, denoised_image\n",
    "    else:\n",
    "        print(\"Failed to capture an image.\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1367: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvWaitKey'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Capture an image, remove the background, and preprocess it\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m input_image, preprocessed_image \u001b[38;5;241m=\u001b[39m capture_and_preprocess()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Check if the preprocessed image was successfully captured\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preprocessed_image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Display the input image\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 13\u001b[0m, in \u001b[0;36mcapture_and_preprocess\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Allow the camera to adjust (you may need to adjust the delay based on your camera)\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Read a frame from the camera\u001b[39;00m\n\u001b[0;32m     16\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1367: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvWaitKey'\n"
     ]
    }
   ],
   "source": [
    "# Capture an image, remove the background, and preprocess it\n",
    "input_image, preprocessed_image = capture_and_preprocess()\n",
    "\n",
    "# Check if the preprocessed image was successfully captured\n",
    "if preprocessed_image is not None:\n",
    "    # Display the input image\n",
    "    display(IPImage(data=cv2.imencode('.png', input_image)[1].tobytes(), format='png'))\n",
    "\n",
    "    # Display the preprocessed image\n",
    "    display(IPImage(data=cv2.imencode('.png', preprocessed_image)[1].tobytes(), format='png'))\n",
    "\n",
    "    print(\"Image captured, background removed, and preprocessed.\")\n",
    "else:\n",
    "    print(\"Image capture failed.\")\n",
    "\n",
    "\n",
    "# Assuming 'preprocessed_image' is your preprocessed image\n",
    "input_array = np.expand_dims(preprocessed_image, axis=0)\n",
    "\n",
    "# Make predictions on the input image\n",
    "predictions = cnn_model.predict(input_array)\n",
    "\n",
    "\n",
    "\n",
    "# Get the class labels\n",
    "class_labels = list(train_generator.class_indices.keys())\n",
    "\n",
    "# Find the class label with the highest probability\n",
    "predicted_class_index = np.argmax(predictions)\n",
    "predicted_class_label = class_labels[predicted_class_index]\n",
    "predicted_probability = predictions[0, predicted_class_index]\n",
    "\n",
    "# Print the predicted class label and its probability\n",
    "print(f\"Predicted Class - {predicted_class_label}, Probability - {predicted_probability:.4f}\")\n",
    "\n",
    "\n",
    "print(predicted_class_label)\n",
    "\n",
    "text_input = predicted_class_label\n",
    "audio_output = text_to_speech(text_input)\n",
    "\n",
    "audio_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
